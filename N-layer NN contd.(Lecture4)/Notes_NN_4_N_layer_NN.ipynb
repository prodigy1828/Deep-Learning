{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "- **Recap**\n",
        "\n",
        "- **MLP: Multi layer Preceptron**\n",
        "\n",
        "- **Activation Functions**\n",
        "\n",
        "- **Notations**\n"
      ],
      "metadata": {
        "id": "mGZSOfaR7isg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap"
      ],
      "metadata": {
        "id": "xF8uZE9SWkcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We made a simple Neural network to classify a  multiclass setting use case"
      ],
      "metadata": {
        "id": "FvAGvcyxXR8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1-f3HjcdZ_K4SeT3P8pr7Bp_zFjJixXXi' width=\"800\"></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QoOEHBNMtqlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Revision) Question: How many weights did we have for 2 feature input and 3 neurons?\n",
        "\n",
        "Ans: Since we have 2 input and 3 neuron, we have total of 2*3 = 6 weights and 3 bias (one per neuron.)"
      ],
      "metadata": {
        "id": "onTb6K8GDcA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Revision) What activation did we use in output ?\n",
        "\n",
        "Ans: Softmax"
      ],
      "metadata": {
        "id": "9OBG5-3qEBzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we only had a simple network of 3 neuron.\n",
        "\n",
        "#### What if we add more neuron to our network and make it more complex ?\n",
        "\n",
        "Let's see how we do it"
      ],
      "metadata": {
        "id": "miLPyASltzx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP: Multi Layer Perceptron"
      ],
      "metadata": {
        "id": "Nm1EUxZdDsTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We make our model/ network complex by adding a layer between our input and output."
      ],
      "metadata": {
        "id": "6SIvTk5_Lmop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1wMWkvsARzV1eN0JpUcgg1J63dJkkcUAD' width=\"800\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "e4kQZtezLt0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's understand what we did here.\n",
        "\n",
        "#### What are we given with?\n",
        "\n",
        "- We know that we have 2 feature input and\n",
        "- we have 3 classes in output.\n",
        "\n"
      ],
      "metadata": {
        "id": "rBy7Ro-wMVPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understanding the architecture\n",
        "\n",
        "We took our 2 inputs and connected them with the the 4 neuron of Layer-1\n",
        "- These 4 neurons will be connected with the 3 neuron of the output layer.\n",
        "\n",
        "This intermediate layer in between the input and output layer is called **Hidden layer**\n",
        "\n"
      ],
      "metadata": {
        "id": "KQJGCJYeMrvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### But, why is called hidden layer?\n",
        "\n",
        "As both input and output are hidden from this layer,\n",
        "and\n",
        "- we don't directly deal with this layer,\n",
        "- hence, it is called hidden layer.\n",
        "\n",
        "We make our network complex by increasing the number of neuron in the layer or by increasing the number of layers."
      ],
      "metadata": {
        "id": "Ip73Rut6PvmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: What will be the activation function for output layer?\n",
        "\n",
        "Ans: Since it is a multiclass classification problem, we'll use softmax activation function"
      ],
      "metadata": {
        "id": "Rtjvq23hqOAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What will be the activation function for hidden layer ?\n",
        "\n",
        "Recall that we want our model to learn non linear decision boundary,\n",
        "- Hence, we'll always use non linear activation function\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mQi26y-Adx8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: What happens if each activation function in MLP is a linear function?"
      ],
      "metadata": {
        "id": "922BK2gpmHEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to understand it using an example"
      ],
      "metadata": {
        "id": "W9W8CkSKk_nJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRtHGWr3lJU9"
      },
      "source": [
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Case 1:**\n",
        "\n",
        "Given $f(x) = 2x+1, g(x) = 3x+2$, will $f(g(x))$ be linear or non-linear?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1TNF_X-rg9Wee84yURhbc8AfAE0T758f4' width=\"800\"></center>\n"
      ],
      "metadata": {
        "id": "y4m-x1Y1GkeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f(g(x)) = 2(3x+2) + 1 = 6x + 5$ ---> linear\n",
        "\n",
        "- Composition of two linear function is linear.\n",
        "\n",
        "If we use linear activation\n",
        "- it'll become a linear model.\n",
        "\n",
        "Remember, we have to generare higher order non-linear features\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "A4J-JX1JGhCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How non-linear function helps us here ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Bg08QrImyzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Case 2:** $f(x) = x^2+1, g(x) = 2x+1$\n",
        "\n"
      ],
      "metadata": {
        "id": "PGSm2cBJGzfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1hZsaEgqvksWKed5pcAtNeGpEWTMexZTA' width=\"800\"></center>\n"
      ],
      "metadata": {
        "id": "0iLAP0rBHirX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f(g(x)) = (2x+1)^2 + 1 = 4x^2 + 2x + 2$ ---> non-linear, higher order\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "l3nTM6vzHhWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q. How about we stack non-linearity once more?\n",
        "\n"
      ],
      "metadata": {
        "id": "mPKWUXO4Gvy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 3:** $h(x) = x^2+1, f(x) = 2x^2+1, g(x) = x+1$\n"
      ],
      "metadata": {
        "id": "_1fjW79TIxas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1IDG5hLnzrQvKUE0PKgaxr-6_2vcqrWn7' width=\"800\"></center>\n"
      ],
      "metadata": {
        "id": "IexqaIlRJP6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$f(h(g(x))) = 2((x+1)^2+1)^2 + 1 =  2(2x^2 + 2x + 3)^2 + 1$ ---> non-linear, much higher order\n",
        "\n",
        "<br>\n",
        "\n",
        "Conclusion - **Stacking a non-linearity over a linear function, and repeating the process may create complex features**"
      ],
      "metadata": {
        "id": "DNj8pV4eJOpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, suppose we have 2 inputs: $x_1, x_2$\n",
        "\n",
        "They are going through the following **computation graph**.\n",
        "\n",
        "  <center><img src='https://drive.google.com/uc?id=1zVmAdtYjnAiZ0trhE2XV9pSrOw9fFOF9' width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "0gtJg-c_EUNr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M57VBrYT0CF7"
      },
      "source": [
        "- $z$ is a weighted sum, so it'll be linear only: $z = 3x_1 + 4x_2$\n",
        "- $a_1$ will be non-linear and complex: $a_1 = 9x_1^2 + 16x_2^2 + 24x_1x_2$\n",
        "- $a_2$ will be non-linear and much more complex: $a_2 = (9x_1^2 + 16x_2^2 + 24x_1x_2 +1)^2 = (9x_1^2 + 16x_2^2)^2 + 2(9x_1^2 + 16x_2^2)(24x_1x_2 +1) + (24x_1x_2 +1)^2$\n",
        "\n",
        "<hr style=\"border:1px solid gray\"> </hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What non-linear function shall we use?  Have you seen any non linear function in the ML algos so far ?\n",
        "\n",
        "\n",
        "Ans: Recall Sigmoid is one of the non linear activation function we saw.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qzxqJcCHipyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: Should we use different activation functions in different layers?\n",
        "\n",
        "***Anwser:***\n",
        "- Theoretically we can. But studies have shown that it is not of much value.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cLp_JIjxrU-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation function"
      ],
      "metadata": {
        "id": "bG9aCo9Tiq8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw that how sigmoid can be used as a activation function for hidden layer."
      ],
      "metadata": {
        "id": "H6520VEBq6o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, Sigmoid activation function was very popular in 1980's and 1990's"
      ],
      "metadata": {
        "id": "QR9hrhLztjRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1YsuzkFwJUxGkAtjV8_QYnui4Sx_kqurC'>"
      ],
      "metadata": {
        "id": "saagnU2hs4oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Are there any other activation functions ?"
      ],
      "metadata": {
        "id": "hlxH5PBZr-gQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tanh function\n",
        "\n",
        "- Shifted version of sigmoid function\n",
        "- Works better than sigmoid almost all the time - mean value is zero\n",
        "- Inputs lies in the range: $(- ‚àû, ‚àû)$\n",
        "- Output lies in the range: $(-1, 1)$\n",
        "- We don't use tanh function very often, unless we want output to lie in the range of (-1, 1).\n",
        "- Formula: $tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
        "\n",
        "- Derivative of tanh function: $\\frac{d(tanh(z))}{dz} = 1-tanh^2(z)$\n",
        " - This lies between $(0, 1)$\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1M2i6y-FQd9mmsxK1jNPyVicfe3ETZvq5)"
      ],
      "metadata": {
        "id": "5eIlR72-q5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Issue with Sigmoid and tanh"
      ],
      "metadata": {
        "id": "KPgBL3nStazI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vanishing Gradients"
      ],
      "metadata": {
        "id": "ZxW1jv5aq5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downside of both sigmoid and tanh is that their gradient is ~0, for most of the values of z**\n",
        "\n",
        "- This hampers the gradient descent process, and the calculated gradients will be very small.\n",
        "\n",
        "#### Why does small gradient hampers GD process ?\n",
        "\n",
        "Let's understand this with an example.\n",
        "\n",
        "Consider a 3-layered NN, that has activation functions as sigmoid or tanh, and looks like:-"
      ],
      "metadata": {
        "id": "zfFRGjWGq5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1wqf5JdPOwD6GrH1w4XiqLAufx1OSdtYF)\n",
        "\n"
      ],
      "metadata": {
        "id": "EmV_XGMFt4AB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we wish to update the value of $w_{11}^1$\n",
        "\n",
        "From backward propagation algo, we know that we need to calculate the partial derivative of loss with respect to this weight, as:\n",
        "\n",
        "$\\frac{‚àÇL}{‚àÇw^1_{11}}$ = $\\frac{‚àÇL}{‚àÇa^3_1}[\\frac{‚àÇa^3_1}{‚àÇa^2_{11}}.\\frac{‚àÇa^2_{11}}{‚àÇa^1_{11}}.\\frac{‚àÇa^1_{11}}{‚àÇw^1_{11}}$ +\n",
        " $\\frac{‚àÇa^3_1}{‚àÇa^2_{21}}.\\frac{‚àÇa^2_{21}}{‚àÇa^1_{12}}.\\frac{‚àÇa^1_{12}}{‚àÇw^1_{11}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "- Since the activation functions are sigmoid or tanh\n",
        "- We know that derivative of these functions lie between (0, 1).\n",
        "- So, the product of these terms inside the bracket, will become very small.\n",
        "- In fact, as the number of layers in the NN increase, this product will become smaller and smaller.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FdwxIBrGq5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1Q0WfF4zXKVJxCsZpzBgjCY_wofro964e)"
      ],
      "metadata": {
        "id": "SLFuNcCAwdpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The equation to update the weight $w_{11}^1$ is: $w^1_{11_{new}} = w^1_{11_{new}} - ùù∂.\\frac{‚àÇ \\ loss}{‚àÇ \\ w_{ij}^k} \\biggr\\vert_{{W^k_{ij}}_{old}}$\n",
        "\n",
        "- We just saw that this partial derivative value becomes miniscule, as the number of layers increase.\n",
        "- As a result, the NN gets trained very very slowly.\n",
        "- In fact, for close to 2 decades, people could not imagine using a NN with more than 3-4 layers.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JrTPBP3Qq5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=17jxwftkYEValZB0zZGk-3iu2QpvrWkGE)\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "y07g_tMQUsfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How shall we deal with this problem ?  "
      ],
      "metadata": {
        "id": "vJK4rGSHw6_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ReLu\n",
        "\n",
        "- Stands for **Rectified Linear Unit**.\n",
        "- It says, that for any positive value, it will return that value as it is: $Relu(z) = z$, if $z > 0$\n",
        "- Otherwise, it returns 0: $Relu(z) = 0$, if $z <= 0$\n",
        "\n",
        "It can be stated as: $Relu(z) = max(z, 0)$\n",
        "- Very fast to compute.\n",
        "\n",
        "- Derivative of ReLu wrt z: $ReLu'(z) = \\left\\{\\begin{matrix}\n",
        "1, if \\ z>0\\\\\n",
        "0, if \\ z<0\n",
        "\\end{matrix}\\right.$\n",
        "\n",
        " - Though it is not differential at 0, as it is not continuous\n",
        " - So here we take an approximation, for it to work. So we make it as:$ReLu'(z) = \\left\\{\\begin{matrix}\n",
        "1, if \\ z>0\\\\\n",
        "0, if \\ z<=0\n",
        "\\end{matrix}\\right.$\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=11WM7DXTHZNFI06flIdUcR-zKbn8n1YBz)"
      ],
      "metadata": {
        "id": "7MR8e9k6q5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: Is there a problem in practically using ReLu as activation function?\n",
        "\n",
        "Yes. Even though it is the most widely used activation function in the world of Deep Learning, there is a slight problem.\n",
        "\n",
        "\n",
        "- If even one derivative term in calculation of $\\frac{\\partial Loss}{‚àÇ w}$ gets the value as 0, the entire term will become zero.\n",
        "\n",
        "- Hence, there is no update in the value of weight.\n",
        "- This is also know as **\"dying ReLU\"**\n",
        "- So there's a potential **vanishing gradient** problem.\n",
        "    - While calculating backprop, if one of the derivative is 0, the whole update will become zero and network won't update\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "However, to deal with this, a slight modification is made to ReLu, and we get another activation function known as **Leaky ReLu**\n",
        "\n"
      ],
      "metadata": {
        "id": "y1C7qTOrq5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=1l4UnOtfTGexfm2gJEj4sTnHwAzzym1mR)"
      ],
      "metadata": {
        "id": "7DbC82VaxJay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Leaky ReLu\n",
        "\n",
        "- This is very similar to ReLu but there's a twist\n",
        "- In case of negative vales, we add a small gradient ($Œ±$) associated with it, instead of having 0.\n",
        "- Gradient: $Leaky \\ ReLu'(z) = \\left\\{\\begin{matrix}\n",
        "1, if \\ z>0\\\\\n",
        "Œ±, if \\ z<=0\n",
        "\\end{matrix}\\right.$\n",
        "\n"
      ],
      "metadata": {
        "id": "F4bUrVu5q5Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1pbkhu3NxintyCdHo5DSVy6M6aQaDhbcI)"
      ],
      "metadata": {
        "id": "1s67e4Bsq5Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notations"
      ],
      "metadata": {
        "id": "RK-cZ904qRhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Revision) Question: What was the notation for weights and bias ?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FHjKzPTNT9tV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1-f3HjcdZ_K4SeT3P8pr7Bp_zFjJixXXi' width=\"800\"></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sa5Jq6A0SmJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:\n",
        "\n",
        "For weight: $w_{ij}$\n",
        "\n",
        "where\n",
        "- i represents from neuron\n",
        "- j represent to neuron\n",
        "\n",
        "<br>\n",
        "\n",
        "Bias: $b_i$\n",
        "where\n",
        "- i represents the neuron."
      ],
      "metadata": {
        "id": "KjUV44h9R_uF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But these notations won't work when it comes to N layered NN.\n",
        "- So, we introduce layer number in notation\n",
        "    - as layer increase, weight notation will become repetitive"
      ],
      "metadata": {
        "id": "dloOzLLyTN25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "For weight: $w^L_{ij}$\n",
        "\n",
        "where\n",
        "- L represent layer number\n",
        "- and i represents from neuron\n",
        "- j represent to neuron\n",
        "\n",
        "<br>\n",
        "\n",
        "Bias: $b^L_i$\n",
        "where\n",
        "- L represents the layer\n",
        "- i represents the neuron."
      ],
      "metadata": {
        "id": "04JInT7ySCcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1LAnU_9qgGqf9LbK1PzotW6J80WRYb9Rx' width=\"800\"></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aX18RkNPbgOI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BY45N9trEWD"
      },
      "source": [
        "\n",
        "\n",
        "We will now have two sets of weights and biases (for the first and second layers) - $W^1, W^2, b^1, b^2$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### What will be the dimensions of the parameters $W^1, b^1, W^2, b^2$?\n",
        "\n",
        "\n",
        "Forget about the second layer for now, let's just focus on the first layer, and it's inputs.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### $W^1$\n",
        "- Here, we have **2 inputs** being passed to **4 neurons**\n",
        "- Recall that each pair of input and neuron, has a unique weights value.\n",
        "- Therefore, $W^1$ will be $2\\times4$ matrix:\n",
        "$W^1 = \\begin{bmatrix}\n",
        "w_{11}^1 & w_{12}^1 & w_{13}^1 & w_{14}^1\\\\\n",
        "w_{21}^1 & w_{22}^1 & w_{23}^1 & w_{24}^1\n",
        "\\end{bmatrix}_{2 \\times 4}$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### $b^1$\n",
        "- There are 4 neurons in the first layer.\n",
        "- Each of which has an unique bias value.\n",
        "- Therefore, $b^1$ will be $1 \\times 4$ matrix: $b^1 = \\begin{bmatrix}\n",
        "b_1^1 & b_2^1 & b_3^1 & b_4^1\n",
        "\\end{bmatrix}_{1 \\times 4}$\n"
      ],
      "metadata": {
        "id": "eG79j4RS0j4z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED3RYU5-2GUV"
      },
      "source": [
        "Let's figure dimensions $W^2$ and $b^2$ by considering only layer-1 and layer-2 again as a one layer NN\n",
        "\n",
        "\n",
        "#### $W^2$\n",
        "- Now, the outputs of the layer-1 are inputs to the layer-2.\n",
        "- Layer-2 neurons don't need to know if its an output from an earlier layer, or a raw input\n",
        "- Here, 4 inputs are passed to 3 neurons, $W^2$ will be $4\\times3$ matrix: $W^2 = \\begin{bmatrix}\n",
        "w_{11}^2 & w_{12}^2 & w_{13}^2\\\\\n",
        "w_{21}^2 & w_{22}^2 & w_{23}^2\\\\\n",
        "w_{31}^2 & w_{32}^2 & w_{33}^2\\\\\n",
        "w_{41}^2 & w_{42}^2 & w_{43}^2\n",
        "\\end{bmatrix}_{4 \\times 3}$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### $b^2$\n",
        "- There are 3 neurons in second layer, each of which will have a bias associated with them.\n",
        "- Therefore, $b^2$ will be $1 \\times 3$ matrix: $b^2 = \\begin{bmatrix}\n",
        "b_1^2 & b_2^2 & b_3^2\n",
        "\\end{bmatrix}_{1 \\times 3}$\n",
        "\n"
      ]
    }
  ]
}